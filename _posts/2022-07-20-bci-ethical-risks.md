---
layout: post
title: "Brain-Computer Interfaces (BCI) - Ethical Risks"
tags:
- Data Science
- Thoughts
- BCI
- Ethics
- AI
---

Brain-computer interfaces enable direct communication between the brain and the computer. The technology can record neuro data or modulate neuro data or do both. In addition, it is an AI system that recognizes specific patterns in our brain signals. Let's focus on BCIs that are powered by electroencephalography (EEG) technology, which is non-invasive. In a BCI, neural oscillations can be measured, processed, stored, and analyzed using both hardware and software components. The direct link between the brain and the computer makes it possible for humans to influence or be influenced by digital systems without being physically bound. 
 
BCI is a dual-use technology that can both enhance our lives and be misused. One of the critical socio-legal aspects of BCI technology is around the regulation of the technology and to what extent the regulatory boards and public health authorities endorse that BCIs are safe, bio-compatible, and effective for non-therapeutic purposes for various age groups. Some of the socio-legal aspects to consider around these features are soft laws, biometric data collection laws, corporate self-governance, and upstream governance. Furthermore, it is important to consider who is liable/responsible for human actions assisted/influenced by BCI (if a user did something illegal under the influence of BCI, who would be liable for it? Are such legal risks foreseen?), and what laws apply to testing and consumer protection around such technologies & commercialization. We could also consider this socio-tech aspect around the viability/existence of frameworks or guidelines around how BCI infrastructure/systems are designed and to what extent we can govern and self-regulate it. 
 
Considering the inherent features of BCI technology, some of the generic ethical issues/risks include: (1) Challenging the notion that humans are not meant to be controlled by machines, (2) BCI can be harmful to the evolution of humanity if we increasingly rely on it to assist/enhance our abilities (3) The rich & powerful will get unfair access to these technologies, resulting in an uneven playing field for all. (4) BCIs can pose serious threats to human rights, privacy, agency, and autonomy. The expectations of the technology may not match with certain groups of populations, which may open the risk of psychological harm. Additionally, it's essential to recognize that the features of the technology itself may or may not be well understood by the users who consent to using or testing these devices. 
 
We cannot disregard the concerns around the dual-use technology or process-related artifacts arising from BCI tech; for example, (1) BCI technology is likely to lead to applications/uses that harm privacy, and so privacy should be considered in the design and architecture level. (2) It may lead to entertainment (and other) applications that can be morally unacceptable or only accessible to certain socioeconomic populations, widening inequalities. (3) The data retention and processing strategies/laws around the raw data collected from these devices may not be wholly shaped and can be challenging to self-regulate (4) Algorithmic biases emerging from the artifacts can also further widen inequalities and can marginalize the use of such technology (5) Causal agency without legal agency when we cannot directly hold any particular entity liable and when we cannot foresee the risks around BCI machine learning algorithms which can be unpredictable in nature. (6) Brain enhancement beyond what "normal" behavior looks like and potential mental changes in users under the influence of BCI. Here is a handful of uses of the technology that might raise the most concern: (1)Hacking BCI devices/artifacts/systems can compromise sensitive & personal information (2) Using BCI to manipulate someone or influence them to do/cause harm can violate human rights, (3) Long-term effects and implications of using BCI to regulate certain emotions like fear, for example, in the military settings. (4) Addictive nature of BCIs for entertainment purposes can then lead to withdrawal symptoms and other socio-ethical challenges. (5) The loss of human authenticity, autonomy, and agency resulting from excessive BCI use. The paradox of BCI is that it gives the user more control, while simultaneously increasing their control over the device. It can prove to be very dangerous if the artifacts start blurring the line between assistive applications of BCI and enhancing applications of BCI. If the technology is hacked or otherwise misused, the psychological and physical harm caused by the artifacts can have negative socio-tech consequences. The neuro data collected from BCI is very biometric and granular in nature, and it can magnify privacy risks. The long-term irreversible damages resulting from any BCI applications and their side effects seem pretty scary and unknown (in terms of psychological, physical, and privacy harms).

We should actively consider proactive measures and privacy-by-design approaches that can help with the control/alignment problem of BCIs. Some of the design opportunities when here could be - (1) Thinking about embedding privacy in the design of the BCI headset so that the data is anonymized/encrypted when it's being sent to the remote server (2) Warnings/lockout to allow users to take a break between prolonged usage for their wellbeing (3) Preventing against hardware manipulation/hacking or malfunction (4) Enforcing strict access rules and restrictions around how the data and neural signals get used, retained, and processed. (5) Retaining only the data the application needs for the required time. (6) Employ federated learning, anonymization, and PETs to safeguard data (7) Ensure data quality, testing, and statistical parity around various machine learning algorithms utilized as part of the BCI system/stack (8) On-off switches when appropriate to stop recording neuro data (9) Use adaptive algorithms and differential privacy approaches to both improve the accuracy of decisions based on the data and balance utility with privacy. (10) Identifying the risks, disclosing them, and providing readable documentation/training should be part of the packaging and the initial onboarding process. Developers should think hard about notice and consent at every lifecycle. Privacy by architecture here could provide a higher level of privacy and reliability to users without the need to analyze or otherwise negotiate lengthy privacy policies. As a whole, these design opportunities should directly address ethical concerns around fairness, the gap between user expectations and reality, the right to understanding, remedy access, and ensure data security and privacy.
