---
layout: post
title: "Reinforcement Fine-Tuning (RFT)"
tags:
- Reward Engineering
- Evals
- OpenAI
- AI
- RFT
---

The standard RLHF stack optimizes a murky averaged-out “human preference.” RFT drops the guesswork. You write a grader that can say *yes or no*—did the SQL query return the right rows, did the tool call succeed, did the proof type-check? That single bit of ground-truth signal lets a model climb fast with only dozens to thousands of graded examples instead of the millions of ranked pairs traditional RLHF chews through. OpenAI’s early RFT users are already turning narrow domain models into near-expert systems on a weekend’s worth of data and a cloud bill you can put on a corporate card ([OpenAI][1], [TechRadar][2]).

Verifiability is the unlock. Agentic tasks—planning, function calling, program synthesis—produce outputs that are either correct or not. There’s no middle, no taste-checking panel, just binary truth. That makes reward hacking obvious and fixable, and it keeps training runs honest in a way preference modeling never could ([Louis-François Bouchard][3], [YouTube][4]).

The craft that matters now is **rubric engineering**: short, deterministic graders or LLM critics that enforce the contract you actually care about. A ten-line pytest-style script can replace thousands of labeled rows; tweak the rubric, re-train overnight, and see the lift next morning. In practice, most of the iteration time moves from data collection to tightening those assertions. Treat the rubric like production code—version-control it, review it, roll back if it sends your model in the wrong direction ([Louis-François Bouchard][3]).

Start every RFT project with a brutal eval. If experts don’t unanimously agree on the answer, or if the base model is already perfect (or hopeless), you’re wasting cycles. The sweet spot is where humans nail 100%, frontier models hit 60–80%, and the gap is costing real money or credibility. RFT then becomes mechanical engineering: improve reward, retrain, re-measure ([Interconnects][5], [Medium][6]).

Accuracy isn’t the whole game. Targeted RFT runs often shrink chain-of-thought sprawl and shave hundreds of milliseconds off first-token latency; in production that can dwarf the training bill. When you’re doing millions of calls a day, compressing capability into a smaller model pays for itself in weeks, not quarters ([OpenAI][7], [OpenAI Help Center][8]).

Zoom out five years: generic frontier models will keep climbing, but specialist RFT-compressed models will run the critical paths—clearing trades, signing legal briefs, triaging medical images. Reward engineering will look a lot like writing unit tests today, and shipping an agent without a rubric-backed training loop will feel reckless. The era of prompt-and-pray is closing; the era of reward-programmed software is here ([Interconnects][9]).
